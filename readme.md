#[Twitch Spell Check Challenge](http://www.twitch.tv/problems/spellcheck)#

##License:##
  [Academic Free License ("AFL") v. 3.0](http://opensource.org/licenses/AFL-3.0)

##Usage:##
  To compile the programs, just type `make` and the makefile will produce the two executables. 
  If you want to produce the debug executables, type `make debug`. 
  You can also remove the executables and related output files by typing `make clean`. 
  The programs make heavy use of C++11 features, so it is recommended that the programs are compiled on OS X Mountain Lion with the latest version of the developer tools installed (more information can be found inside the makefile).

  To verify the programs, execute the verification script (`./verification.sh`). This action will produce two additional files called wordsgenerated.txt and verificationresults.txt. 
  The former contains the words generated by the word generator and the latter contains the output of the spell checker.

##About the Storage:##
  For this challenge, storing the dictionary efficiently is part of how you keep your runtime below O(n) where n is the length of the dictionary. 
  Originally I thought that a hash table would be a good idea as each word could be hashed and then stored in 26 buckets. 
  However, the search time is on average O(1 + n/k), where k is the number of buckets. 
  I decided that maybe something could be faster. After looking over binary search trees and red-black trees, I ran across the trie. 
  Tries store information alphabetically, which is much more helpful than the pseudo-random order of hash tables, there are no collisions, and there is no need for a hash function. 
  In addition, tries are more space efficient since nodes are shared between words with common letters. 
  Since nodes are shared, this allows for longest prefix matching which means you can search using an approximate matching algorithm. 
  With these properties, search time is at worst O(m), where m is the length of the key (or in this case, the word being looked for).

##Runtime Complexity:##
  To lookup words, I elected to use edit distance as a basis for the search algorithm. First, I change the word into all lowercase, which is O(m). 
  Then I search the trie for the word without any other changes, which is O(m). 
  If the word is not found in the trie, I remove the duplicate characters in the word (which is another O(m) operation), then I recursively search the trie again with the cost of O(m) for each substring. 
  Finally, I replace the vowels in the word (O(m) cost) and recursively search (O(m) cost per substring) 5 separate times, each time being with a different vowel. 
  However, each recursive search is limited by a distance parameter passed into the function to limit how far away from the word the algorithm searches. 
  Since all of the operations are O(m), the runtime complexity looks like it is O(km), where k is a nonzero constant and, in this case, k is the number of searches plus the cost of manipulating characters in the string. 
  However, because O(km) = O(m) if k is nonzero, the runtime of the program is O(m). 

##Other Thoughts:##
  The trie can be simplified further since all that is being stored is characters. 
  A directed acyclic word graph (DAWG) compresses identical branches of a trie which lead to the same suffixes. 
  Another option is to convert the trie into a bitwise trie and do all operations using bits. 
  This would allow for parallelized searching and can be executed quickly on out-of-order CPUs using special instructions in some of the recent instruction sets. 
  It is also possible to change the implementation to solely rely on Levenshtein Distance, but the runtime may get slower as you have to iterate through the dictionary calculating distances and then check whether or not it is below the maximum threshold.

  Another thing is that as is, my spelling corrector is dumb as it solely relies on distance and has no context to find results. 
  This leads to a large number of possible corrections which are equally valid replacements based on the 3 rules of the problem. 
  To fix this, it would be interesting to "train" the corrector by feeding it large amounts of text to learn about the words it has in its dictionary, then search using Bayes' Theorem. 
  Granted, this would increase the pre-processing time and the memory used, but it would produce more accurate results. 
  Currently, I output at most 10 suggestions returned from the search function as the problem states that there doesn't have to be only one output (it says can). 
  Since the search function can return a lot of results, sometimes the most likely word isn't output to the user (it is still in the set of results), but it is better than being bombarded by hundreds of possible suggestions. 
  In interest of full disclosure, it is possible for the generator program to pick a word from the dictionary and mangle it so much, that the distance limit I have set in the search function will not be able to reconstruct the word. 
  To alleviate this, the limit can be increased, but that will increase the amount of time taken to search for a given word and increase the number of possible corrections returned.

